{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashnahari/AIResearchColab/blob/main/Copy_of_transformer_asr_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcl7Lb3G0kYZ"
      },
      "source": [
        "# Automatic Speech Recognition with Transformer\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2021/01/13<br>\n",
        "**Last modified:** 2021/01/13<br>\n",
        "**Description:** Training a sequence-to-sequence Transformer for automatic speech recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEYDJG6A0kYb"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n",
        "ASR can be treated as a sequence-to-sequence problem, where the\n",
        "audio can be represented as a sequence of feature vectors\n",
        "and the text as a sequence of characters, words, or subword tokens.\n",
        "\n",
        "For this demonstration, we will use the LJSpeech dataset from the\n",
        "[LibriVox](https://librivox.org/) project. It consists of short\n",
        "audio clips of a single speaker reading passages from 7 non-fiction books.\n",
        "Our model will be similar to the original Transformer (both encoder and decoder)\n",
        "as proposed in the paper, \"Attention is All You Need\".\n",
        "\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/pdf/1904.13377.pdf)\n",
        "- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n",
        "- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YOxiyLim0kYb"
      },
      "outputs": [],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "CBl6nNyB_xMi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiYm87Ss0kYb"
      },
      "source": [
        "## Define the Transformer Input Layer\n",
        "\n",
        "When processing past target tokens for the decoder, we compute the sum of\n",
        "position embeddings and token embeddings.\n",
        "\n",
        "When processing audio features, we apply convolutional layers to downsample\n",
        "them (via convolution strides) and process local relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fuqNtihn0kYc"
      },
      "outputs": [],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, num_vocab=31, maxlen=100, num_hid=32):\n",
        "        super().__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid, mask_zero=True)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "\n",
        "\n",
        "class SpeechFeatureEmbedding(layers.Layer):\n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 70, strides=1, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 70, strides=1, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 70, strides=1, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.conv3(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yotT4cZ40kYc"
      },
      "source": [
        "## Transformer Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g_WPi-Fb0kYc"
      },
      "outputs": [],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AvoBvZw0kYc"
      },
      "source": [
        "## Transformer Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KO8H9qCd0kYc"
      },
      "outputs": [],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.self_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.self_dropout = layers.Dropout(0.5)\n",
        "        self.enc_dropout = layers.Dropout(0.1)\n",
        "        self.ffn_dropout = layers.Dropout(0.1)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
        "\n",
        "        This prevents flow of information from future tokens to current token.\n",
        "        1's in the lower triangle, counting from the lower right corner.\n",
        "        \"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_out, target):\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
        "        ffn_out = self.ffn(enc_out_norm)\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
        "        return ffn_out_norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2koARei0kYd"
      },
      "source": [
        "## Complete the Transformer model\n",
        "\n",
        "Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n",
        "During training, we give the decoder the target character sequence shifted to the left\n",
        "as input. During inference, the decoder uses its own past predictions to predict the\n",
        "next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a_aXrdjn0kYd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.gen_data_flow_ops import parallel_dynamic_stitch\n",
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "class Transformer(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=32,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        source_maxlen=100,\n",
        "        target_maxlen=100,\n",
        "        num_layers_enc=4,\n",
        "        num_layers_dec=1,\n",
        "        num_classes=31,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.accuracy_metric = keras.metrics.Accuracy()\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
        "        )\n",
        "\n",
        "        self.encoder = keras.Sequential(\n",
        "            [self.enc_input]\n",
        "            + [\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "                for _ in range(num_layers_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(\n",
        "                self,\n",
        "                f\"dec_layer_{i}\",\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
        "            )\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "    def decode(self, enc_out, target):\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs):\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source)\n",
        "        y = self.decode(x, target)\n",
        "        z = self.classifier(y)\n",
        "        return z\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric, self.accuracy_metric]\n",
        "\n",
        "    def predMask(self, preds):\n",
        "        preds = tf.cast(preds, tf.int32)\n",
        "\n",
        "        # Create a tensor of the same shape filled with the value to find\n",
        "        to_find = tf.fill(tf.shape(preds), 0)\n",
        "\n",
        "        # Create a mask where the tensor is equal to the value to find\n",
        "        mask = tf.equal(preds, to_find)\n",
        "\n",
        "        # Get the index of the first occurrence of the value for each row\n",
        "        indices = tf.argmax(tf.cast(mask, tf.int32), axis=-1)\n",
        "\n",
        "        indices = tf.cast(indices, tf.int32)\n",
        "\n",
        "        # Create a sequence of indices for each row\n",
        "        row_indices = tf.range(tf.shape(preds)[1])\n",
        "\n",
        "        # Broadcast the row indices to the same shape as the tensor\n",
        "        row_indices = tf.broadcast_to(row_indices, tf.shape(preds))\n",
        "\n",
        "        row_indices = tf.cast(row_indices, tf.int32)\n",
        "\n",
        "        # Create a mask that is 1 for all indices up to the first occurrence for each row\n",
        "        mask_up_to_first_occurrence = row_indices <= indices[:, None]\n",
        "\n",
        "        # Cast the boolean mask to integers\n",
        "        mask_up_to_first_occurrence = tf.cast(mask_up_to_first_occurrence, tf.int32)\n",
        "\n",
        "        return tf.cast(mask_up_to_first_occurrence,tf.bool)\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            predsMaxed = tf.argmax(preds, axis=-1)\n",
        "            dec_target = tf.cast(dec_target, dtype=tf.int32)\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            targetMask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "            predMask = self.predMask(predsMaxed)\n",
        "            mask = tf.math.logical_or(targetMask, predMask)\n",
        "\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        self.accuracy_metric.update_state(dec_target, predsMaxed, sample_weight=mask)\n",
        "        return {\"loss\": self.loss_metric.result(), \"accuracy\": self.accuracy_metric.result()}\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, batch):\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        dec_target = tf.cast(dec_target, dtype=tf.int32)\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        predsMaxed = tf.argmax(preds, axis=-1)\n",
        "        targetMask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "        predMask = self.predMask(predsMaxed)\n",
        "        mask = tf.math.logical_or(targetMask, predMask)\n",
        "\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        self.accuracy_metric.update_state(dec_target, predsMaxed, sample_weight=mask)\n",
        "        return {\"loss\": self.loss_metric.result(), \"accuracy\": self.accuracy_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "\n",
        "        return dec_input\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waT98iR2Bk6q",
        "outputId": "f85c4d6d-340b-4386-8c53-bec96d5c7a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/my_drive; to attempt to forcibly remount, call drive.mount(\"/content/my_drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/my_drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L08OJxet0kYd"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "Note: This requires ~3.6 GB of disk space and\n",
        "takes ~5 minutes for the extraction of files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k9Ol87rW6ty9"
      },
      "outputs": [],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "def normalizeSentenceDataCube(sentenceDat, singleLetterDat):\n",
        "    \"\"\"\n",
        "    Normalizes the neural data cube by subtracting means and dividing by the standard deviation.\n",
        "    Important: we use means and standard deviations from the single letter data. This is needed since we\n",
        "    initialize the HMM parameters using the single letter data, so the sentence data needs to be normalized in the same way.\n",
        "    \"\"\"\n",
        "    neuralCube = sentenceDat['neuralActivityCube'].astype(np.float64)\n",
        "\n",
        "    #subtract block-specific means from each trial to counteract the slow drift in feature means over time\n",
        "    for b in range(sentenceDat['blockList'].shape[0]):\n",
        "        trialsFromThisBlock = np.squeeze(sentenceDat['sentenceBlockNums']==sentenceDat['blockList'][b])\n",
        "        trialsFromThisBlock = np.argwhere(trialsFromThisBlock)\n",
        "\n",
        "        closestIdx = np.argmin(np.abs(singleLetterDat['blockList'].astype(np.int32) - sentenceDat['blockList'][b].astype(np.int32)))\n",
        "        blockMeans = singleLetterDat['meansPerBlock'][closestIdx,:]\n",
        "\n",
        "        neuralCube[trialsFromThisBlock,:,:] -= blockMeans[np.newaxis,np.newaxis,:]\n",
        "\n",
        "    #divide by standard deviation to normalize the units\n",
        "    neuralCube = neuralCube / singleLetterDat['stdAcrossAllData'][np.newaxis,:,:]\n",
        "\n",
        "    return neuralCube\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XMEdo6lY692S"
      },
      "outputs": [],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "def binTensor(data, binSize):\n",
        "    \"\"\"\n",
        "    A simple utility function to bin a 3d numpy tensor along axis 1 (the time axis here). Data is binned by\n",
        "    taking the mean across a window of time steps.\n",
        "\n",
        "    Args:\n",
        "        data (tensor : B x T x N): A 3d tensor with batch size B, time steps T, and number of features N\n",
        "        binSize (int): The bin size in # of time steps\n",
        "\n",
        "    Returns:\n",
        "        binnedTensor (tensor : B x S x N): A 3d tensor with batch size B, time bins S, and number of features N.\n",
        "                                           S = floor(T/binSize)\n",
        "    \"\"\"\n",
        "\n",
        "    nBins = np.floor(data.shape[1]/binSize).astype(int)\n",
        "\n",
        "    sh = np.array(data.shape)\n",
        "    sh[1] = nBins\n",
        "    binnedTensor = np.zeros(sh)\n",
        "\n",
        "    binIdx = np.arange(0,binSize).astype(int)\n",
        "    for t in range(nBins):\n",
        "        binnedTensor[:,t,:] = np.mean(data[:,binIdx,:],axis=1)\n",
        "        binIdx += binSize;\n",
        "\n",
        "    return binnedTensor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLA3Ws5L0kYd"
      },
      "source": [
        "## Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hPSsr45O3W9t"
      },
      "outputs": [],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "\n",
        "class VectorizeChar:\n",
        "    def __init__(self, max_len=50):\n",
        "        self.vocab = (\n",
        "            [\"]\", \"[\"]\n",
        "            + [chr(i + 96) for i in range(1, 27)]\n",
        "            + [\">\", \",\", \"'\", \"~\", \"?\"]\n",
        "        )\n",
        "        self.max_len = max_len\n",
        "        self.char_to_idx = {}\n",
        "        for i, ch in enumerate(self.vocab):\n",
        "            self.char_to_idx[ch] = i\n",
        "\n",
        "    def __call__(self, text):\n",
        "        text = text.lower()\n",
        "        text = text[: self.max_len - 2]\n",
        "        text = \"[\" + text + \"]\"\n",
        "        pad_len = self.max_len - len(text)\n",
        "        return [self.char_to_idx.get(ch, 28) for ch in text] + [0] * pad_len\n",
        "\n",
        "    def get_vocabulary(self):\n",
        "        return self.vocab\n",
        "\n",
        "\n",
        "max_target_len = 200\n",
        "vectorizer = VectorizeChar(max_target_len)\n",
        "\n",
        "\n",
        "def create_tf_dataset(neural_ds, text_ds, bs=4):\n",
        "\n",
        "    ds = tf.data.Dataset.zip((neural_ds, text_ds))\n",
        "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
        "    # ds = ds.batch(bs)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adoFOaLqDxrF",
        "outputId": "97afeece-8795-4d87-8f66-6b49c033b64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['w', 'e', '>', 't', 'r', 'i', 'e', 'd', '>', 'e', 'v', 'e', 'r', 'y', 't', 'h', 'i', 'n', 'g', '>', 'e', 'l', 's', 'e', '~']\n"
          ]
        }
      ],
      "source": [
        "x = [24, 6, 28, 21, 19, 10, 6, 5, 28, 6, 23, 6, 19, 26, 21, 9, 10, 15, 8, 28, 6, 13, 20, 6, 31]\n",
        "v = vectorizer.get_vocabulary()\n",
        "print([v[c] for c in x])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m73cgkAX3f2",
        "outputId": "5d384473-94eb-424b-e63f-b437deba09e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 24, 6, 28, 21, 19, 10, 6, 5, 28, 6, 23, 6, 19, 26, 21, 9, 10, 15, 8, 28, 6, 13, 20, 6, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "vectorizer2 = VectorizeChar(50)\n",
        "print(vectorizer2(\"we tried everything else~\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1seCP7SGwdis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2fed96-181a-4e03-f47c-771a200a9656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "neural data:\n",
            "\n",
            "11\n",
            "20\n",
            "25\n",
            "27\n",
            "46\n",
            "48\n",
            "50\n",
            "51\n",
            "56\n",
            "58\n",
            "2\n",
            "20\n",
            "35\n",
            "42\n",
            "43\n",
            "55\n",
            "56\n",
            "77\n",
            "21\n",
            "22\n",
            "27\n",
            "46\n",
            "79\n",
            "80\n",
            "4\n",
            "8\n",
            "17\n",
            "23\n",
            "27\n",
            "61\n",
            "76\n",
            "78\n",
            "13\n",
            "14\n",
            "20\n",
            "24\n",
            "41\n",
            "46\n",
            "48\n",
            "70\n",
            "76\n",
            "5\n",
            "8\n",
            "13\n",
            "34\n",
            "39\n",
            "41\n",
            "48\n",
            "71\n",
            "80\n",
            "9\n",
            "25\n",
            "30\n",
            "38\n",
            "40\n",
            "48\n",
            "54\n",
            "74\n",
            "81\n",
            "4\n",
            "9\n",
            "10\n",
            "19\n",
            "21\n",
            "53\n",
            "54\n",
            "69\n",
            "72\n",
            "15\n",
            "20\n",
            "41\n",
            "59\n",
            "61\n",
            "64\n",
            "67\n",
            "77\n",
            "99\n",
            "11\n",
            "20\n",
            "25\n",
            "27\n",
            "46\n",
            "48\n",
            "50\n",
            "51\n",
            "56\n",
            "58\n",
            "2\n",
            "20\n",
            "35\n",
            "42\n",
            "43\n",
            "55\n",
            "56\n",
            "77\n",
            "21\n",
            "22\n",
            "27\n",
            "46\n",
            "79\n",
            "80\n",
            "4\n",
            "8\n",
            "17\n",
            "23\n",
            "27\n",
            "61\n",
            "76\n",
            "78\n",
            "13\n",
            "14\n",
            "20\n",
            "24\n",
            "41\n",
            "46\n",
            "48\n",
            "70\n",
            "76\n",
            "5\n",
            "8\n",
            "13\n",
            "34\n",
            "39\n",
            "41\n",
            "48\n",
            "71\n",
            "80\n",
            "9\n",
            "25\n",
            "30\n",
            "38\n",
            "40\n",
            "48\n",
            "54\n",
            "74\n",
            "81\n",
            "4\n",
            "9\n",
            "10\n",
            "19\n",
            "21\n",
            "53\n",
            "54\n",
            "69\n",
            "72\n",
            "15\n",
            "20\n",
            "41\n",
            "59\n",
            "61\n",
            "64\n",
            "67\n",
            "77\n",
            "99\n",
            "\n",
            "strings:\n",
            "\n",
            "11\n",
            "20\n",
            "25\n",
            "27\n",
            "46\n",
            "48\n",
            "50\n",
            "51\n",
            "56\n",
            "58\n",
            "2\n",
            "20\n",
            "35\n",
            "42\n",
            "43\n",
            "55\n",
            "56\n",
            "77\n",
            "21\n",
            "22\n",
            "27\n",
            "46\n",
            "79\n",
            "80\n",
            "4\n",
            "8\n",
            "17\n",
            "23\n",
            "27\n",
            "61\n",
            "76\n",
            "78\n",
            "13\n",
            "14\n",
            "20\n",
            "24\n",
            "41\n",
            "46\n",
            "48\n",
            "70\n",
            "76\n",
            "5\n",
            "8\n",
            "13\n",
            "34\n",
            "39\n",
            "41\n",
            "48\n",
            "71\n",
            "80\n",
            "9\n",
            "25\n",
            "30\n",
            "38\n",
            "40\n",
            "48\n",
            "54\n",
            "74\n",
            "81\n",
            "4\n",
            "9\n",
            "10\n",
            "19\n",
            "21\n",
            "53\n",
            "54\n",
            "69\n",
            "72\n",
            "15\n",
            "20\n",
            "41\n",
            "59\n",
            "61\n",
            "64\n",
            "67\n",
            "77\n",
            "99\n",
            "11\n",
            "20\n",
            "25\n",
            "27\n",
            "46\n",
            "48\n",
            "50\n",
            "51\n",
            "56\n",
            "58\n",
            "2\n",
            "20\n",
            "35\n",
            "42\n",
            "43\n",
            "55\n",
            "56\n",
            "77\n",
            "21\n",
            "22\n",
            "27\n",
            "46\n",
            "79\n",
            "80\n",
            "4\n",
            "8\n",
            "17\n",
            "23\n",
            "27\n",
            "61\n",
            "76\n",
            "78\n",
            "13\n",
            "14\n",
            "20\n",
            "24\n",
            "41\n",
            "46\n",
            "48\n",
            "70\n",
            "76\n",
            "5\n",
            "8\n",
            "13\n",
            "34\n",
            "39\n",
            "41\n",
            "48\n",
            "71\n",
            "80\n",
            "9\n",
            "25\n",
            "30\n",
            "38\n",
            "40\n",
            "48\n",
            "54\n",
            "74\n",
            "81\n",
            "4\n",
            "9\n",
            "10\n",
            "19\n",
            "21\n",
            "53\n",
            "54\n",
            "69\n",
            "72\n",
            "15\n",
            "20\n",
            "41\n",
            "59\n",
            "61\n",
            "64\n",
            "67\n",
            "77\n",
            "99\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import scipy.ndimage.filters\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "sentencesFilenames = ['/content/my_drive/MyDrive/AIResearch/sentencesFile_0.mat', '/content/my_drive/MyDrive/AIResearch/sentencesFile_1.mat', '/content/my_drive/MyDrive/AIResearch/sentencesFile_2.mat', '/content/my_drive/MyDrive/AIResearch/sentencesFile_3.mat','/content/my_drive/MyDrive/AIResearch/sentencesFile_4.mat', '/content/my_drive/MyDrive/AIResearch/sentencesFile_5.mat', '/content/my_drive/MyDrive/AIResearch/sentencesFile_6.mat', '/content/my_drive/MyDrive/AIResearch/sentencesFile_7.mat', '/content/my_drive/MyDrive/AIResearch/sentencesFile_8.mat']\n",
        "\n",
        "singleLettersFilenames = ['/content/my_drive/MyDrive/AIResearch/singleLettersFile_0.mat','/content/my_drive/MyDrive/AIResearch/singleLettersFile_1.mat', '/content/my_drive/MyDrive/AIResearch/singleLettersFile_2.mat', '/content/my_drive/MyDrive/AIResearch/singleLettersFile_3.mat','/content/my_drive/MyDrive/AIResearch/singleLettersFile_4.mat', '/content/my_drive/MyDrive/AIResearch/singleLettersFile_5.mat', '/content/my_drive/MyDrive/AIResearch/singleLettersFile_6.mat', '/content/my_drive/MyDrive/AIResearch/singleLettersFile_7.mat', '/content/my_drive/MyDrive/AIResearch/singleLettersFile_8.mat']\n",
        "\n",
        "epochs = 10000\n",
        "random.seed(73)\n",
        "randomList = random.choices(range(0, 100), k=90)\n",
        "\n",
        "def neural_generator(sentenceFiles, slFiles, randomlist):\n",
        "       for i in range(len(sentenceFiles)):\n",
        "          # Open and read the file\n",
        "          sentenceFile = sentenceFiles[i]\n",
        "          slFile = slFiles[i]\n",
        "\n",
        "\n",
        "\n",
        "          sentenceFile = scipy.io.loadmat(sentenceFile)\n",
        "          slFile = scipy.io.loadmat(slFile)\n",
        "\n",
        "          neuralData = normalizeSentenceDataCube(sentenceFile, slFile)\n",
        "          neuralData = binTensor(neuralData, 2) #RNN BIN SIZE\n",
        "\n",
        "          timePadding = 7000-len(neuralData[1])\n",
        "          paddedNeuralData = tf.keras.layers.ZeroPadding1D(padding=(0,timePadding))(neuralData)\n",
        "\n",
        "          randList = randomlist[i*10:(i+1)*10]\n",
        "\n",
        "          for j in range(len(paddedNeuralData)):\n",
        "            batch = paddedNeuralData[j]\n",
        "            if j in randList:\n",
        "              tf.print(j)\n",
        "              yield batch, 'val'\n",
        "            else:\n",
        "                yield batch, 'train'\n",
        "\n",
        "tf.print(\"\\nneural data:\\n\")\n",
        "neuralData = tf.data.Dataset.from_generator(\n",
        "    lambda: neural_generator(sentencesFilenames, singleLettersFilenames, randomList),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(7000,192), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.string)# Adjust the shape and dtype as per your data\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_data = list(x[0] for x in filter(lambda x: x[1] == 'train', neuralData))\n",
        "test_data = list(x[0] for x in filter(lambda x: x[1] == 'val', neuralData))\n",
        "\n",
        "\n",
        "train_data_ds = tf.data.Dataset.from_tensor_slices(train_data)\n",
        "test_data_ds = tf.data.Dataset.from_tensor_slices(test_data)\n",
        "\n",
        "\n",
        "def strings_generator(sentenceFiles, randomlist):\n",
        "      for i in range(len(sentenceFiles)):\n",
        "          file = sentenceFiles[i]\n",
        "          file = scipy.io.loadmat(file)\n",
        "\n",
        "          strings = np.array([[item[0] for item in arr] for arr in file['intendedText']])\n",
        "          strings = strings.flatten()\n",
        "\n",
        "          randList = randomlist[i*10:(i+1)*10]\n",
        "\n",
        "          for j in range(len(strings)):\n",
        "            sentence = strings[j]\n",
        "            sentence = vectorizer(sentence)\n",
        "            if j in randList:\n",
        "                tf.print(j)\n",
        "                yield sentence, 'val'\n",
        "            else:\n",
        "                yield sentence, 'train'\n",
        "\n",
        "tf.print(\"\\nstrings:\\n\")\n",
        "\n",
        "strings = tf.data.Dataset.from_generator(\n",
        "    lambda: strings_generator(sentencesFilenames, randomList),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.string)# Adjust the shape and dtype as per your data\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_strings = list(x[0] for x in filter(lambda x: x[1] == 'train', strings))\n",
        "test_strings = list(x[0] for x in filter(lambda x: x[1] == 'val', strings))\n",
        "\n",
        "\n",
        "train_strings_ds = tf.data.Dataset.from_tensor_slices(train_strings)\n",
        "test_strings_ds = tf.data.Dataset.from_tensor_slices(test_strings)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j7QdobfJSAgE"
      },
      "outputs": [],
      "source": [
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.9, val_split=0.1, shuffle=False):\n",
        "    assert (train_split + val_split) == 1\n",
        "\n",
        "    if shuffle:\n",
        "        # Specify seed to always have the same split distribution between runs\n",
        "        ds = ds.shuffle(ds_size, seed=12)\n",
        "\n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "\n",
        "    train_ds = ds.take(train_size)\n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "\n",
        "    return train_ds, val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KMgNw8Meo3vx"
      },
      "outputs": [],
      "source": [
        "ds = create_tf_dataset(train_data_ds, train_strings_ds)\n",
        "val_ds = create_tf_dataset(test_data_ds, test_strings_ds)\n",
        "\n",
        "# dataset = create_tf_dataset(neuralData, strings)\n",
        "\n",
        "\n",
        "# ds = ds.shuffle(buffer_size=100000000)\n",
        "\n",
        "numBatch = 845  #845 for full dataset\n",
        "\n",
        "# ds, val_ds = get_dataset_partitions_tf(dataset, numBatch)\n",
        "\n",
        "# ds = ds.repeat(epochs)\n",
        "\n",
        "ds = ds.batch(1)\n",
        "val_ds = val_ds.batch(1)\n",
        "\n",
        "# del train_data\n",
        "# del test_data\n",
        "# del train_strings\n",
        "# del test_strings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Tc5kIg0kYd"
      },
      "source": [
        "## Callbacks to display predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zLhkjDV_0kYd"
      },
      "outputs": [],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "\n",
        "class DisplayOutputs(keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self, batch, idx_to_token, target_start_token_idx=0, target_end_token_idx=1,\n",
        "    ):\n",
        "        \"\"\"Displays a batch of outputs after every epoch\n",
        "\n",
        "        Args:\n",
        "            batch: A test batch containing the keys \"source\" and \"target\"\n",
        "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
        "            target_start_token_idx: A start token index in the target vocabulary\n",
        "            target_end_token_idx: An end token index in the target vocabulary\n",
        "        \"\"\"\n",
        "        self.batch = batch\n",
        "        self.target_start_token_idx = target_start_token_idx\n",
        "        self.target_end_token_idx = target_end_token_idx\n",
        "        self.idx_to_char = idx_to_token\n",
        "\n",
        "    def on_epoch_end(self,epoch, logs=None):\n",
        "        source = self.batch[\"source\"]\n",
        "        target = self.batch[\"target\"].numpy()\n",
        "        bs = tf.shape(source)[0]\n",
        "        preds = self.model.generate(source, self.target_start_token_idx)\n",
        "        preds = preds.numpy()\n",
        "        for i in range(bs):\n",
        "            target_indices = target.astype(int)\n",
        "            target_text = \"\".join([self.idx_to_char[_] for _ in target_indices[i, :]])\n",
        "            prediction = \"\"\n",
        "            target_text = target_text[0:target_text.index(self.idx_to_char[self.target_end_token_idx],1)+1]\n",
        "            for idx in preds[i, :]:\n",
        "                prediction += self.idx_to_char[idx]\n",
        "                if idx == self.target_end_token_idx:\n",
        "                    break\n",
        "\n",
        "            target_text = target_text.replace('>',' ')\n",
        "            prediction = prediction.replace('>',' ')\n",
        "            print(\"\\n\")\n",
        "            print(f\"target:     {target_text.replace('~','.')}\")\n",
        "            print(f\"prediction: {prediction}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcuBRlvy0kYd"
      },
      "source": [
        "## Learning rate schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tdmlmpPl0kYd"
      },
      "outputs": [],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "\n",
        "class CustomSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        init_lr=0.00001,\n",
        "        lr_after_warmup=0.001,\n",
        "        final_lr=0.00001,\n",
        "        warmup_epochs=15,\n",
        "        decay_epochs=85,\n",
        "        steps_per_epoch=203,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.init_lr = tf.constant(init_lr, dtype=tf.float32)\n",
        "        self.lr_after_warmup = tf.constant(lr_after_warmup, dtype=tf.float32)\n",
        "        self.final_lr = tf.constant(final_lr, dtype=tf.float32)\n",
        "        self.warmup_epochs = tf.constant(warmup_epochs, dtype=tf.float32)\n",
        "        self.decay_epochs = tf.constant(decay_epochs, dtype=tf.float32)\n",
        "        self.steps_per_epoch = tf.constant(steps_per_epoch, dtype=tf.float32)\n",
        "\n",
        "\n",
        "    def calculate_lr(self, epoch):\n",
        "        \"\"\" linear warm up - linear decay \"\"\"\n",
        "        warmup_lr = (\n",
        "            self.init_lr\n",
        "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
        "        )\n",
        "        decay_lr = tf.math.maximum(\n",
        "            self.final_lr,\n",
        "            tf.cast(self.lr_after_warmup\n",
        "            - (epoch - self.warmup_epochs)\n",
        "            * (self.lr_after_warmup - self.final_lr)\n",
        "            / self.decay_epochs,dtype=tf.float32),\n",
        "        )\n",
        "        return tf.cast(tf.math.minimum(warmup_lr, decay_lr), dtype=tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        epoch = tf.cast(step, tf.float32) // self.steps_per_epoch\n",
        "        return tf.cast(self.calculate_lr(epoch), dtype=tf.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(\n",
        "    init_lr=1e-5,\n",
        "    lr_after_warmup=1e-3,\n",
        "    final_lr=1e-5,\n",
        "    warmup_epochs=5,\n",
        "    decay_epochs=100,\n",
        "    steps_per_epoch=numBatch,\n",
        ")\n",
        "\n",
        "epochs=10000\n",
        "lrs = [learning_rate(i) for i in range(epochs)]\n",
        "\n",
        "\n",
        "plt.plot(range(epochs),lrs)\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Learning Rate Schedule\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "30-K0FGoB_-W",
        "outputId": "d2bb1a9c-b351-418b-af95-8cc7007f446f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLlie6mo0kYe"
      },
      "source": [
        "In practice, you should train for around 100 epochs or more.\n",
        "\n",
        "Some of the predicted text at or around epoch 35 may look as follows:\n",
        "```\n",
        "target:     <as they sat in the car, frazier asked oswald where his lunch was>\n",
        "prediction: <as they sat in the car frazier his lunch ware mis lunch was>\n",
        "\n",
        "target:     <under the entry for may one, nineteen sixty,>\n",
        "prediction: <under the introus for may monee, nin the sixty,>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z_vlpZ07as2"
      },
      "source": [
        "## Create & train the end-to-end model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "3fssmWIOnHZ-",
        "outputId": "425a3076-bdc4-4904-d478-7ab24fc99d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-cc5e8be990cb>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#model.evaluate(x=ds,callbacks=[display_cb], return_dict=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#model.load_weights(checkpoint_filepath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdisplay_cb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m       _, _, filtered_flat_args = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#---------------\n",
        "#      RUN\n",
        "#---------------\n",
        "\n",
        "iterator = iter(val_ds)\n",
        "batch = next(iterator)\n",
        "\n",
        "\n",
        "# The vocabulary to convert predicted indices into characters\n",
        "idx_to_char = vectorizer.get_vocabulary()\n",
        "display_cb = DisplayOutputs(\n",
        "    batch, idx_to_char, target_start_token_idx=1, target_end_token_idx=0\n",
        ")\n",
        "\n",
        "model = Transformer(\n",
        "    num_hid=32,\n",
        "    num_head=2,\n",
        "    num_feed_forward=400,\n",
        "    target_maxlen=max_target_len,\n",
        "    num_layers_enc=4,\n",
        "    num_layers_dec=1,\n",
        "    num_classes=31,\n",
        ")\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=True, label_smoothing=0.1,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = CustomSchedule(\n",
        "    init_lr=1e-5,\n",
        "    lr_after_warmup=1e-3,\n",
        "    final_lr=1e-5,\n",
        "    warmup_epochs=5,\n",
        "    decay_epochs=100,\n",
        "    steps_per_epoch=numBatch,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "checkpoint_filepath = os.path.join('/content/my_drive/MyDrive/AIResearch/outputDir','ckpt','weights.ckpt')\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    verbose=1)\n",
        "\n",
        "log_dir = \"/content/my_drive/MyDrive/AIResearch/outputDir-logs/logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "#model.evaluate(x=ds,callbacks=[display_cb], return_dict=True)\n",
        "#model.load_weights(checkpoint_filepath)\n",
        "model.fit(ds, validation_data=val_ds, callbacks=[display_cb, model_checkpoint_callback,tensorboard_callback], epochs=epochs, initial_epoch=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /content/my_drive/MyDrive/AIResearch/outputDir-logs/logs/fit"
      ],
      "metadata": {
        "id": "e_x8AlLY_-At"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}